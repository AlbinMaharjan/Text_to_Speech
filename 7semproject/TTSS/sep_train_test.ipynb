{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eutai file ma name ra data lyako"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved at: merged_filename_phonemes.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "filename_df = pd.read_csv('filename.csv')  # Replace with the actual path to 'filename.csv'\n",
    "phoneme_df = pd.read_csv('padded_phoneme_sequences.csv')  # Replace with the actual path to 'padded_phoneme_sequences.csv'\n",
    "\n",
    "# Merge the datasets by adding filenames to the padded phoneme sequences\n",
    "merged_df = pd.DataFrame({\n",
    "    'filename': filename_df['filename'],\n",
    "    'padded_phonemes': phoneme_df['padded_phonemes']\n",
    "})\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "output_path = 'merged_filename_phonemes.csv'  # Update the path accordingly\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file with lengths saved at: merged_filename_phonemes_with_lengths.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "filename_df = pd.read_csv('filename.csv')  # Replace with the actual path to 'filename.csv'\n",
    "phoneme_df = pd.read_csv('padded_phoneme_sequences.csv')  # Replace with the actual path to 'padded_phoneme_sequences.csv'\n",
    "\n",
    "# Assuming the 'padded_phonemes' column contains lists or sequences, compute their lengths\n",
    "phoneme_df['length'] = phoneme_df['padded_phonemes'].apply(lambda x: len(eval(x)) if isinstance(x, str) else len(x))\n",
    "\n",
    "# Merge the datasets by adding filenames to the padded phoneme sequences\n",
    "merged_df = pd.DataFrame({\n",
    "    'filename': filename_df['filename'],\n",
    "    'padded_phonemes': phoneme_df['padded_phonemes'],\n",
    "    'length': phoneme_df['length']\n",
    "})\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "output_path = 'merged_filename_phonemes_with_lengths.csv'  # Update the path accordingly\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file with lengths saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to D:\\7semproject\\TTSS\\combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "phoneme_file = r'D:\\7semproject\\TTSS\\merged_filename_phonemes_with_lengths.csv'\n",
    "text_file = r'D:\\7semproject\\TTSS\\raw_text.csv'\n",
    "output_file = r'D:\\7semproject\\TTSS\\combined_data.csv'\n",
    "\n",
    "# Load CSV files\n",
    "phoneme_df = pd.read_csv(phoneme_file)\n",
    "text_df = pd.read_csv(text_file)\n",
    "\n",
    "# Merge on the 'filename' column\n",
    "combined_df = pd.merge(phoneme_df, text_df, on='filename', how='inner')\n",
    "\n",
    "# Rename columns to match the required format\n",
    "combined_df = combined_df[['filename', 'transcription', 'padded_phonemes', 'length']]\n",
    "\n",
    "# Save to new CSV file\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Merged data saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined audio metadata saved to D:\\7semproject\\TTSS\\combined_audio_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "raw_audio_dir = r'D:\\7semproject\\LjSpeech\\wavs'\n",
    "processed_audio_dir = r'D:\\7semproject\\TTSS\\log_scaled_mel_spectrograms'\n",
    "mel_lengths_path = r'D:\\7semproject\\TTSS\\mel_lengths.csv'\n",
    "\n",
    "# Read mel lengths CSV\n",
    "mel_lengths_df = pd.read_csv(mel_lengths_path)\n",
    "\n",
    "# Ensure 'filename' column exists in the CSV\n",
    "if 'filename' in mel_lengths_df.columns:\n",
    "    # Remove any row where 'filename' is 'max_mel_length' to avoid extra entry\n",
    "    mel_lengths_df = mel_lengths_df[mel_lengths_df['filename'] != 'max_mel_length']\n",
    "\n",
    "    # Remove '.npy' extension from the filename column if present\n",
    "    mel_lengths_df['filename'] = mel_lengths_df['filename'].str.replace('.npy', '', regex=False)\n",
    "\n",
    "    # Add columns for full paths to raw and processed audio files\n",
    "    mel_lengths_df['raw_audio_path'] = mel_lengths_df['filename'].apply(lambda x: os.path.join(raw_audio_dir, f\"{x}.wav\"))\n",
    "    mel_lengths_df['processed_audio_path'] = mel_lengths_df['filename'].apply(lambda x: os.path.join(processed_audio_dir, f\"{x}.npy\"))\n",
    "\n",
    "    # Reorder columns for clarity\n",
    "    mel_lengths_df = mel_lengths_df[['filename', 'raw_audio_path', 'processed_audio_path', 'mel_length']]\n",
    "\n",
    "    # Save the combined data to a new CSV file\n",
    "    output_csv = r'D:\\7semproject\\TTSS\\combined_audio_metadata.csv'\n",
    "    mel_lengths_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Combined audio metadata saved to {output_csv}\")\n",
    "else:\n",
    "    print(\"Error: 'filename' column not found in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13100 common filenames. Proceeding with merge.\n",
      "Merged DataFrame has 13100 rows.\n",
      "Merged dataset saved to merged_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "audio_metadata = pd.read_csv('combined_audio_metadata.csv')\n",
    "\n",
    "# Check for common filenames\n",
    "common_filenames = set(combined_data['filename']).intersection(set(audio_metadata['filename']))\n",
    "\n",
    "if not common_filenames:\n",
    "    print(\"No common filenames found between the two files.\")\n",
    "    print(f\"Unique filenames in combined_data: {combined_data['filename'].nunique()}\")\n",
    "    print(f\"Unique filenames in audio_metadata: {audio_metadata['filename'].nunique()}\")\n",
    "    print(\"Sample filenames from combined_data:\", combined_data['filename'].head().tolist())\n",
    "    print(\"Sample filenames from audio_metadata:\", audio_metadata['filename'].head().tolist())\n",
    "else:\n",
    "    print(f\"Found {len(common_filenames)} common filenames. Proceeding with merge.\")\n",
    "\n",
    "    # Merge the dataframes based on the common filenames\n",
    "    merged_df = pd.merge(combined_data, audio_metadata, on='filename', how='inner')\n",
    "    print(f\"Merged DataFrame has {merged_df.shape[0]} rows.\")\n",
    "\n",
    "    # Define the output path for the merged dataset\n",
    "    output_csv = 'merged_dataset.csv'\n",
    "    \n",
    "    # Save the merged dataframe to a CSV file\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged dataset saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 10611 samples\n",
      "Validation set: 1179 samples\n",
      "Test set: 1310 samples\n",
      "Metadata files created: train.txt, val.txt, test.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset from a .txt file\n",
    "input_file = r'D:\\Project\\list.txt'  # Replace with the path to your .txt file\n",
    "df = pd.read_csv(input_file, sep='|', names=['raw_audio_path', 'processed_audio_path', 'transcription', 'mel_length'])\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "# Function to create metadata files\n",
    "def create_metadata_file(df, output_path):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            audio_path = row['raw_audio_path']\n",
    "            mel_path = row['processed_audio_path']\n",
    "            text = row['transcription']\n",
    "            mel_length = row['mel_length']\n",
    "            f.write(f\"{audio_path}|{mel_path}|{text}|{mel_length}\\n\")\n",
    "\n",
    "# Paths to save the metadata files\n",
    "train_metadata_path = 'train.txt'\n",
    "val_metadata_path = 'val.txt'\n",
    "test_metadata_path = 'test.txt'\n",
    "\n",
    "# Create metadata files\n",
    "create_metadata_file(train_df, train_metadata_path)\n",
    "create_metadata_file(val_df, val_metadata_path)\n",
    "create_metadata_file(test_df, test_metadata_path)\n",
    "\n",
    "print(f\"Metadata files created: {train_metadata_path}, {val_metadata_path}, {test_metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the list.txt and normalized_text.csv files\n",
    "list_df = pd.read_csv(r'D:\\Project\\list.txt', sep='|', header=None, names=['path', 'transcription'])\n",
    "normalized_df = pd.read_csv(r'D:\\Project\\normalized_text.csv')\n",
    "\n",
    "# Remove the transcription column from list_df\n",
    "list_df = list_df.drop(columns=['transcription'])\n",
    "\n",
    "# Add the normalized_transcription column from normalized_df as the new transcription column\n",
    "list_df['transcription'] = normalized_df['normalized_transcription']\n",
    "\n",
    "# Save the updated DataFrame back to a .txt file with pipe-separated values\n",
    "list_df.to_csv(r'D:\\Project\\updated_list_with_normalized_transcriptions.txt', sep='|', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "audio_metadata = pd.read_csv('combined_audio_metadata.csv')\n",
    "\n",
    "# Check for common filenames\n",
    "common_filenames = set(combined_data['filename']).intersection(set(audio_metadata['filename']))\n",
    "\n",
    "if not common_filenames:\n",
    "    print(\"No common filenames found between the two files.\")\n",
    "    print(f\"Unique filenames in combined_data: {combined_data['filename'].nunique()}\")\n",
    "    print(f\"Unique filenames in audio_metadata: {audio_metadata['filename'].nunique()}\")\n",
    "    print(\"Sample filenames from combined_data:\", combined_data['filename'].head().tolist())\n",
    "    print(\"Sample filenames from audio_metadata:\", audio_metadata['filename'].head().tolist())\n",
    "else:\n",
    "    print(f\"Found {len(common_filenames)} common filenames. Proceeding with merge.\")\n",
    "\n",
    "    # Merge and save merged data if common filenames exist\n",
    "    merged_df = pd.merge(combined_data, audio_metadata, on='filename', how='inner')\n",
    "    print(f\"Merged DataFrame has {merged_df.shape[0]} rows.\")\n",
    "\n",
    "    # Proceed with data split only if merged_df is non-empty\n",
    "    if not merged_df.empty:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        # Split into train, validation, and test sets\n",
    "        train_val, test = train_test_split(merged_df, test_size=0.1, random_state=42)\n",
    "        train, val = train_test_split(train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "        # Save the splits\n",
    "        train.to_csv('train_data.csv', index=False)\n",
    "        val.to_csv('val_data.csv', index=False)\n",
    "        test.to_csv('test_data.csv', index=False)\n",
    "\n",
    "        print(\"Data split into train, validation, and test sets.\")\n",
    "    else:\n",
    "        print(\"The merged DataFrame is empty after attempting to merge.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def check_and_correct_csv(file_path, output_path, expected_columns=8):\n",
    "    \"\"\"\n",
    "    Check and correct the CSV file at the given path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        output_path (str): Path to the output CSV file.\n",
    "        expected_columns (int): The expected number of columns in the CSV.\n",
    "    \"\"\"\n",
    "    corrected_lines = []\n",
    "    problematic_lines = []\n",
    "\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "\n",
    "        for line_number, row in enumerate(reader, start=1):\n",
    "            # Check if the row has the expected number of columns\n",
    "            if len(row) != expected_columns:\n",
    "                if len(row) > expected_columns:\n",
    "                    # Assume the last column contains extra data\n",
    "                    corrected_row = row[:expected_columns - 1] + [' '.join(row[expected_columns - 1:])]\n",
    "                else:\n",
    "                    # Attempt to fill missing values for a row with too few columns\n",
    "                    corrected_row = row + [''] * (expected_columns - len(row))  # Fill with empty strings\n",
    "                \n",
    "                problematic_lines.append((line_number, row, \"Incorrect number of columns, corrected.\"))\n",
    "                corrected_lines.append([f'\"{item}\"' for item in corrected_row])  # Add quotes\n",
    "                continue  # Skip further processing for this row\n",
    "\n",
    "            # Escape unescaped quotes in each element\n",
    "            try:\n",
    "                corrected_row = [f'\"{item.replace(\"\\\"\", \"\\\"\\\"\")}\"' for item in row]\n",
    "                corrected_lines.append(corrected_row)  # Append the corrected row\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the problematic line and the reason for the failure\n",
    "                problematic_lines.append((line_number, row, str(e)))\n",
    "\n",
    "    # Write the corrected lines to a new CSV file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(corrected_lines)\n",
    "\n",
    "    # Output problematic lines for review\n",
    "    if problematic_lines:\n",
    "        print(\"Problematic lines found:\")\n",
    "        for line_number, row, reason in problematic_lines:\n",
    "            print(f\"Line {line_number}: {row} - Reason: {reason}\")\n",
    "\n",
    "# Example usage\n",
    "check_and_correct_csv(\n",
    "    'D:\\\\7semproject\\\\TTSS\\\\text_to_speech_tacotron2\\\\tacotron2\\\\filelists\\\\train_data.csv',\n",
    "    'D:\\\\7semproject\\\\TTSS\\\\text_to_speech_tacotron2\\\\tacotron2\\\\filelists\\\\train_data_corrected.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 'D:\\Project\\list.txt' to UTF-8 and saved as 'D:\\Project\\clist.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def convert_to_utf8(input_file, output_file, source_encoding='ISO-8859-1'):\n",
    "    try:\n",
    "        # Open the input file with the specified source encoding\n",
    "        with open(input_file, 'r', encoding=source_encoding) as infile:\n",
    "            # Read the content\n",
    "            content = infile.read()\n",
    "        \n",
    "        # Write the content to the output file in UTF-8 encoding\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(content)\n",
    "        \n",
    "        print(f\"Successfully converted '{input_file}' to UTF-8 and saved as '{output_file}'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "input_file_path = r'D:\\Project\\list.txt'  # Replace with your input file path\n",
    "output_file_path = r'D:\\Project\\clist.txt'  # Replace with your desired output file path\n",
    "\n",
    "# Specify the source encoding if it's different from ISO-8859-1\n",
    "convert_to_utf8(input_file_path, output_file_path, source_encoding='ISO-8859-1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_to_speech_synthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
