{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "                            normalized_transcription\n",
      "0  Printing, in the only sense with which we are ...\n",
      "1                     in being comparatively modern.\n",
      "2  For although the Chinese took impressions from...\n",
      "3  produced the block books, which were the immed...\n",
      "4  the invention of movable metal letters in the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albin\\AppData\\Local\\Temp\\ipykernel_12572\\2397197750.py:91: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataframe = dataframe.applymap(normalize_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized data saved to normalized_text.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import inflect  # To convert numbers to words\n",
    "import dateparser  # To recognize and parse dates\n",
    "from contractions import fix  # To expand contractions\n",
    "\n",
    "# Initialize inflect engine for number conversion\n",
    "inflect_engine = inflect.engine()\n",
    "\n",
    "# Load the dataset\n",
    "input_file = 'data_text.csv'  # Replace with your file path\n",
    "output_file_normalized = 'normalized_text.csv'  # Output file path for normalized text\n",
    "\n",
    "# Dictionary for common abbreviations\n",
    "abbreviations = {\n",
    "    r'\\be\\.g\\.\\b': 'for example',\n",
    "    r'\\bi\\.e\\.\\b': 'that is',\n",
    "    r'\\betc\\.\\b': 'and so on'\n",
    "}\n",
    "\n",
    "# Function to convert numbers to words\n",
    "def num_to_words(match):\n",
    "    num_str = match.group()\n",
    "    try:\n",
    "        return inflect_engine.number_to_words(int(num_str))\n",
    "    except ValueError:\n",
    "        return num_str\n",
    "\n",
    "# Function to convert date to words\n",
    "def date_to_words(match):\n",
    "    date_str = match.group()\n",
    "    parsed_date = dateparser.parse(date_str)\n",
    "    if parsed_date:\n",
    "        day = inflect_engine.number_to_words(parsed_date.day, ordinal=True)\n",
    "        month = parsed_date.strftime(\"%B\")\n",
    "        year = inflect_engine.number_to_words(parsed_date.year)\n",
    "        return f\"{month} {day} {year}\"\n",
    "    return date_str\n",
    "\n",
    "# Function to handle punctuation and prosody\n",
    "def handle_punctuation(text):\n",
    "    # Remove punctuation except periods\n",
    "    text = re.sub(r'[!?,;:]', '', text)\n",
    "    \n",
    "    # Ensure each line ends with a period\n",
    "    if not text.endswith('.'):\n",
    "        text += '.'\n",
    "        \n",
    "    return text\n",
    "\n",
    "# Function to expand abbreviations\n",
    "def expand_abbreviations(text):\n",
    "    for abbr, expansion in abbreviations.items():\n",
    "        text = re.sub(abbr, expansion, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Function for text normalization\n",
    "def normalize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "        text = fix(text)\n",
    "\n",
    "        # Expand abbreviations (e.g., \"e.g.\" -> \"for example\")\n",
    "        text = expand_abbreviations(text)\n",
    "\n",
    "        # Handle punctuation (preserve periods at the end only)\n",
    "        text = handle_punctuation(text)\n",
    "\n",
    "        # Replace numbers with words (e.g., \"123\" -> \"one hundred twenty-three\")\n",
    "        text = re.sub(r'\\d+', num_to_words, text)\n",
    "\n",
    "        # Replace dates with words (e.g., \"2023-10-04\" -> \"October fourth twenty twenty-three\")\n",
    "        text = re.sub(r'\\b(?:\\d{1,2}[\\/\\-\\s]\\d{1,2}[\\/\\-\\s]\\d{2,4}|\\d{4}[\\/\\-\\s]\\d{1,2}[\\/\\-\\s]\\d{1,2})\\b', date_to_words, text)\n",
    "\n",
    "        # Remove double quotes\n",
    "        text = text.replace('\"', '')\n",
    "\n",
    "        # Remove dashes\n",
    "        text = text.replace('--', '')\n",
    "\n",
    "        # Remove extra whitespace and ensure single spacing between words\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Normalize the entire dataframe\n",
    "def normalize_dataframe(dataframe):\n",
    "    dataframe = dataframe.applymap(normalize_text)\n",
    "    return dataframe\n",
    "\n",
    "try:\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Display the original DataFrame\n",
    "    print(\"Original DataFrame:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Perform normalization on the DataFrame\n",
    "    df_normalized = normalize_dataframe(df)\n",
    "\n",
    "    # Save the normalized DataFrame to a new CSV file\n",
    "    df_normalized.to_csv(output_file_normalized, index=False)\n",
    "    print(f\"\\nNormalized data saved to {output_file_normalized}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHONEME CONVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Albin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme-converted data saved to phoneme_text2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Download CMU Pronouncing Dictionary\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Load CMU Pronouncing Dictionary\n",
    "phoneme_dict = nltk.corpus.cmudict.dict()\n",
    "\n",
    "# Load the normalized dataset\n",
    "input_file_normalized = 'normalized_text.csv'\n",
    "output_file_phonemes = 'phoneme_text2.csv'\n",
    "\n",
    "# Function to convert text to ARPAbet phonemes with fallback for unknown words\n",
    "def text_to_phonemes(text):\n",
    "    if isinstance(text, str):\n",
    "        phoneme_sequence = []\n",
    "        words = text.split()\n",
    "\n",
    "        for word in words:\n",
    "            word_clean = word.strip(string.punctuation)\n",
    "            if word_clean:\n",
    "                word_lower = word_clean.lower()\n",
    "                if word_lower in phoneme_dict:\n",
    "                    # Tacotron2 uses the first pronunciation variant (may need adjustment if multi-variant)\n",
    "                    phonemes_with_stress = phoneme_dict[word_lower][0]\n",
    "                    phoneme_sequence.extend(phonemes_with_stress)\n",
    "                else:\n",
    "                    phoneme_sequence.append(f\"<UNK_{word_clean}>\")  # Unknown word token\n",
    "            if word[-1] in string.punctuation:  # Add pause for punctuation\n",
    "                phoneme_sequence.append('<PAUSE>')\n",
    "        return ' '.join(phoneme_sequence)  # Join phonemes by space for Tacotron2\n",
    "    return text\n",
    "\n",
    "# Apply phoneme conversion\n",
    "try:\n",
    "    df_normalized = pd.read_csv(input_file_normalized)\n",
    "    df_normalized['phoneme_text'] = df_normalized['normalized_transcription'].apply(text_to_phonemes)\n",
    "    df_normalized[['phoneme_text']].to_csv(output_file_phonemes, index=False)\n",
    "    print(f\"Phoneme-converted data saved to {output_file_phonemes}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION giving ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized phoneme data saved to tokenized_phoneme_ids.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the phoneme data\n",
    "input_file_phonemes = 'phoneme_text2.csv'\n",
    "output_file_tokenized = 'tokenized_phoneme_ids.csv'\n",
    "\n",
    "# Define a phoneme-to-ID mapping (ensure this aligns with Tacotron2's expected IDs)\n",
    "phoneme_to_id = {\n",
    "    'AA': 1, 'AE': 2, 'AH': 3, 'AO': 4, 'AW': 5, 'AY': 6, 'B': 7, 'CH': 8, 'D': 9, 'DH': 10,\n",
    "    'EH': 11, 'ER': 12, 'EY': 13, 'F': 14, 'G': 15, 'HH': 16, 'IH': 17, 'IY': 18, 'JH': 19, \n",
    "    'K': 20, 'L': 21, 'M': 22, 'N': 23, 'NG': 24, 'OW': 25, 'OY': 26, 'P': 27, 'R': 28, \n",
    "    'S': 29, 'SH': 30, 'T': 31, 'TH': 32, 'UH': 33, 'UW': 34, 'V': 35, 'W': 36, 'Y': 37, \n",
    "    'Z': 38, 'ZH': 39, '<PAUSE>': 40, '<UNK>': 41\n",
    "}\n",
    "\n",
    "# Function to tokenize phoneme sequences\n",
    "def tokenize_phonemes(phoneme_sequence):\n",
    "    if isinstance(phoneme_sequence, str):\n",
    "        phonemes = phoneme_sequence.split()  # Split by space\n",
    "        token_ids = [phoneme_to_id.get(p, phoneme_to_id['<UNK>']) for p in phonemes]\n",
    "        return token_ids\n",
    "    return []\n",
    "\n",
    "# Apply tokenization\n",
    "try:\n",
    "    df_phonemes = pd.read_csv(input_file_phonemes)\n",
    "    df_phonemes['tokenized_phonemes'] = df_phonemes['phoneme_text'].apply(tokenize_phonemes)\n",
    "    df_phonemes[['tokenized_phonemes']].to_csv(output_file_tokenized, index=False)\n",
    "    print(f\"Tokenized phoneme data saved to {output_file_tokenized}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PADDING AND SEQUENCE LENGTH HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences and lengths saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the tokenized phoneme data\n",
    "phoneme_data = pd.read_csv('tokenized_phoneme_ids.csv')\n",
    "\n",
    "# Function to pad sequences\n",
    "def pad_sequences(sequences, pad_value=0):\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "    lengths = []\n",
    "    for seq in sequences:\n",
    "        padded_seq = seq + [pad_value] * (max_length - len(seq))\n",
    "        padded_sequences.append(padded_seq)\n",
    "        lengths.append(len(seq))  # Store original length\n",
    "    return padded_sequences, torch.tensor(lengths, dtype=torch.int64)\n",
    "\n",
    "# Clean and convert tokenized sequences\n",
    "def clean_and_convert(phoneme_str):\n",
    "    phoneme_list = phoneme_str.strip(\"[]\").replace(\"'\", \"\").split(\",\")\n",
    "    return [int(x.strip()) for x in phoneme_list if x.strip()]\n",
    "\n",
    "# Apply cleaning and padding\n",
    "phoneme_sequences = phoneme_data['tokenized_phonemes'].apply(clean_and_convert)\n",
    "padded_sequences, sequence_lengths = pad_sequences(phoneme_sequences)\n",
    "\n",
    "# Save the padded sequences\n",
    "padded_df = pd.DataFrame({\n",
    "    'padded_phonemes': padded_sequences,\n",
    "    'lengths': sequence_lengths.numpy()\n",
    "})\n",
    "padded_df.to_csv('padded_phoneme_sequences.csv', index=False)\n",
    "\n",
    "print(\"Padded sequences and lengths saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_to_speech_tacotron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
